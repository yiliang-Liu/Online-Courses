{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import Ridge\n",
    "from jupyterthemes import jtplot\n",
    "jtplot.style()\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In this assignment, you will implement ridge regression via gradient descent. You will:\n",
    "#### Convert an SFrame into a Numpy array (if applicable)\n",
    "#### Write a Numpy function to compute the derivative of the regression weights with respect to a single feature\n",
    "#### Write gradient descent function to compute the regression weights given an initial weight vector, step size, tolerance, and L2 penalty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtype_dict = {'bathrooms':float, 'waterfront':int, 'sqft_above':int, 'sqft_living15':float, \n",
    "              'grade':int, 'yr_renovated':int, 'price':float, 'bedrooms':float, 'zipcode':str, \n",
    "              'long':float, 'sqft_lot15':float, 'sqft_living':float, 'floors':str, 'condition':int, \n",
    "              'lat':float, 'date':str, 'sqft_basement':int, 'yr_built':int, 'id':str, 'sqft_lot':int, 'view':int}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Next, from Module 2, copy and paste the ‘get_numpy_data’ function (or equivalent) that takes a dataframe, a list of features (e.g. [‘sqft_living’, ‘bedrooms’]), to be used as inputs, and a name of the output (e.g. ‘price’). This function returns a ‘feature_matrix’ (2D array) consisting of first a column of ones followed by columns containing the values of the input features in the data set in the same order as the input list. It alsos return an ‘output_array’ which is an array of the values of the output in the data set (e.g. ‘price’)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_numpy_data(data_frame, features, output):\n",
    "    \"\"\"\n",
    "    data_frame: pd.Dataframe\n",
    "    featrues: a list of features name (e.g. [‘sqft_living’, ‘bedrooms’])\n",
    "    output: a name of the output (e.g. ‘price’) \n",
    "    \"\"\"\n",
    "    # create a constant column with value one\n",
    "    constant_column = np.ones((len(data_frame), 1))\n",
    "    \n",
    "    # create the features matrix\n",
    "    features_matrix = np.hstack((constant_column, data_frame.as_matrix(columns=features)))\n",
    "    \n",
    "    # this will convert the pd.Series into a numpy array\n",
    "    output_name = [output]\n",
    "    # as_matrix accept list as keywords\n",
    "    output_array = data_frame.as_matrix(columns=output_name)\n",
    "    return features_matrix, output_array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Similarly, copy and paste the ‘predict_output’ function (or equivalent) from Module 2. This function accepts a 2D array ‘feature_matrix’ and a 1D array ‘weights’ and return a 1D array ‘predictions’."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_output(feature_matrix, weights):\n",
    "    return np.dot(feature_matrix, weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  We are now going to move to computing the derivative of the regression cost function. Recall that the cost function is the sum over the data points of the squared difference between an observed output and a predicted output, plus the L2 penalty term.\n",
    "#### Since the derivative of a sum is the sum of the derivatives, we can take the derivative of the first part (the RSS) as we did in the notebook for the unregularized case in Module 2 and add the derivative of the regularization part. As we saw, the derivative of the RSS with respect to w[i] can be written as:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2*SUM[ error * [feature_i] ] + 2*l2_penalty*w[i]\n",
    "##### IMPORTANT: We will not regularize the constant. Thus, in the case of the constant, the derivative is just twice the sum of the errors (without the 2*l2_penalty*w[0] term)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales = pd.read_csv('kc_house_data.csv', dtype=dtype_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_derivative_ridge(errors, feature, weight, l2_penalty, feature_is_constant):\n",
    "    # errors = predictions - output\n",
    "    \n",
    "    if feature_is_constant:\n",
    "        # deal with the intercept, not regularize ir\n",
    "        derivative = 2 * np.dot(feature, errors)\n",
    "    else:\n",
    "        derivative = 2 * np.dot(feature, errors) + 2 * l2_penalty * weight\n",
    "    return derivative"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now we will write a function that performs a gradient descent. The basic premise is simple. Given a starting point we update the current weights by moving in the negative gradient direction. Recall that the gradient is the direction of increase and therefore the negative gradient is the direction of decrease and we're trying to minimize a cost function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### The amount by which we move in the negative gradient direction is called the ‘step size’. We stop when we are ‘sufficiently close’ to the optimum. Unlike in Module 2, this time we will set a maximum number of iterations and take gradient steps until we reach this maximum number. If no maximum number is supplied, the maximum should be set 100 by default. (Use default parameter values in Python.)\n",
    "##### With this in mind, write a gradient descent function using your derivative function above. For each step in the gradient descent, we update the weight for each feature before computing our stopping criteria. The function will take the following parameters:\n",
    "###### 2D feature matrix\n",
    "###### array of output values\n",
    "###### initial weights\n",
    "###### step size\n",
    "###### L2 penalty\n",
    "###### maximum number of iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ridge_regression_gradient_descent(feature_matrix, output, initial_weights, step_size, \n",
    "                                      l2_penalty, max_iterations=100):\n",
    "    # make sure it's a numpy array\n",
    "    weights = np.array(initial_weights).reshape(len(initial_weights), 1).astype(float) \n",
    "    \n",
    "    # while not reached maximum number of iterations:\n",
    "    while max_iterations > 0:\n",
    "        # compute the predictions using your predict_output() function\n",
    "        predictions = predict_output(feature_matrix, weights)\n",
    "        # compute the errors as predictions - output\n",
    "        errors = predictions - output\n",
    "        for i in range(len(weights)): # loop over each weight\n",
    "            # Recall that feature_matrix[:,i] is the feature column associated with weights[i]\n",
    "            # compute the derivative for weight[i].\n",
    "            #(Remember: when i=0, you are computing the derivative of the constant!)\n",
    "            if i == 0:\n",
    "                # pay attention to weights[i]\n",
    "                derivative = feature_derivative_ridge(errors, feature_matrix[:, i], \n",
    "                                                      weights[i], l2_penalty, True)\n",
    "            else:\n",
    "                derivative = feature_derivative_ridge(errors, feature_matrix[:, i], \n",
    "                                                      weights[i], l2_penalty, False)\n",
    "            # subtract the step size times the derivative from the current weight  \n",
    "            weights[i] = weights[i] - step_size * derivative\n",
    "            \n",
    "        max_iterations = max_iterations - 1\n",
    "    return weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv('kc_house_train_data.csv', dtype=dtype_dict)\n",
    "test_data = pd.read_csv('kc_house_test_data.csv', dtype=dtype_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_features = ['sqft_living']\n",
    "my_output = 'price'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "(simple_feature_matrix, output) = get_numpy_data(train_data, simple_features, my_output)\n",
    "(simple_test_feature_matrix, test_output) = get_numpy_data(test_data, simple_features, my_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### First, let’s consider no regularization. Set the L2 penalty to 0.0 and run your ridge regression algorithm to learn the weights of the simple model (described above). Use the following parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "step_size = 1e-12\n",
    "max_iterations = 1000\n",
    "initial_weights = np.array([0, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_weights_0_penalty = ridge_regression_gradient_descent(simple_feature_matrix, output, \n",
    "                                                               initial_weights, step_size, 0, max_iterations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Next, let’s consider high regularization. Set the L2 penalty to 1e11 and run your ridge regression to learn the weights of the simple model. Use the same parameters as above. Call your weights:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_weights_high_penalty = ridge_regression_gradient_descent(simple_feature_matrix, output, \n",
    "                                                               initial_weights, step_size, 1e11, max_iterations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.63113515e-01],\n",
       "       [ 2.63024369e+02]])"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simple_weights_0_penalty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  9.76730382],\n",
       "       [124.57217567]])"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simple_weights_high_penalty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x293a5c52eb8>,\n",
       " <matplotlib.lines.Line2D at 0x293a5c52fd0>,\n",
       " <matplotlib.lines.Line2D at 0x293a5d64160>,\n",
       " <matplotlib.lines.Line2D at 0x293a5d64908>,\n",
       " <matplotlib.lines.Line2D at 0x293a5d64a58>,\n",
       " <matplotlib.lines.Line2D at 0x293a5d72198>]"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x293a50c18d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(simple_feature_matrix,output,'k.',\n",
    "        simple_feature_matrix,predict_output(simple_feature_matrix, simple_weights_0_penalty),'b-',\n",
    "        simple_feature_matrix,predict_output(simple_feature_matrix, simple_weights_high_penalty),'r-')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RSS with weights learned with no regularization:  275723632153607.44\n"
     ]
    }
   ],
   "source": [
    "print(\"RSS with weights learned with no regularization: \", np.sum((test_output - \n",
    "                                      predict_output(simple_test_feature_matrix, simple_weights_0_penalty)) ** 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compute the RSS on the TEST data for the following three sets of weights:\n",
    "##### The initial weights (all zeros)\n",
    "##### The weights learned with no regularization\n",
    "##### The weights learned with high regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_features = ['sqft_living', 'sqft_living15']\n",
    "my_output = 'price'\n",
    "(feature_matrix, output) = get_numpy_data(train_data, model_features, my_output)\n",
    "(test_feature_matrix, test_output) = get_numpy_data(test_data, model_features, my_output)\n",
    "step_size = 1e-12\n",
    "max_iterations = 1000\n",
    "initial_weights = np.array([0, 0, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "multiple_weights_0_penalty = ridge_regression_gradient_descent(feature_matrix, output, \n",
    "                                                               initial_weights, step_size, 0, max_iterations)\n",
    "multiple_weights_high_penalty = ridge_regression_gradient_descent(feature_matrix, output, \n",
    "                                                               initial_weights, step_size, 1e11, max_iterations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ -0.35743483]\n",
      " [243.05416982]\n",
      " [ 22.41481497]]\n"
     ]
    }
   ],
   "source": [
    "print(multiple_weights_0_penalty)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 6.74296579]\n",
      " [91.48927365]\n",
      " [78.43658766]]\n"
     ]
    }
   ],
   "source": [
    "print(multiple_weights_high_penalty)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RSS with initial_weights:  7.545691727070396e+18\n"
     ]
    }
   ],
   "source": [
    "print(\"RSS with initial_weights: \", np.sum((test_output - \n",
    "                                      predict_output(test_feature_matrix, np.array([0, 0, 0]))) ** 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RSS with weights learned with no regularization:  274067615918575.56\n"
     ]
    }
   ],
   "source": [
    "print(\"RSS with weights learned with no regularization: \", np.sum((test_output - \n",
    "                                      predict_output(test_feature_matrix, multiple_weights_0_penalty)) ** 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RSS with weights learned with high regularization:  500404800500841.75\n"
     ]
    }
   ],
   "source": [
    "print(\"RSS with weights learned with high regularization: \", np.sum((test_output - \n",
    "                                      predict_output(test_feature_matrix, multiple_weights_high_penalty)) ** 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predict the house price for the 1st house in the test set using the no regularization and high regularization models. (Remember that python starts indexing from 0.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-77465.47605824])"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_output[0] - predict_output(test_feature_matrix, multiple_weights_0_penalty)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([39546.46967806])"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_output[0] - predict_output(test_feature_matrix, multiple_weights_high_penalty)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
